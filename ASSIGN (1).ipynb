{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4be04c-7acd-482a-9682-5815abf3cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2bcef6-644c-4e08-b1f0-385645c51da1",
   "metadata": {},
   "source": [
    "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables.\n",
    "\n",
    "For instance, when we predict rent based on square feet alone that is Simple linear regression whereas, rent based on square feet and location also is exmaple of Multiple linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded1ac00-b7c5-4876-9b93-7906a12d666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7c4e25-6a65-4517-a8dc-2a86241ae55f",
   "metadata": {},
   "source": [
    "There are primarily five assumptions of linear regression. They are:\n",
    "\n",
    "1) There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "\n",
    "2) Predictors (x) are independent and observed with negligible error\n",
    "\n",
    "3) Residual Errors have a mean value of zero\n",
    "\n",
    "4) Residual Errors have constant variance\n",
    "\n",
    "5) Residual Errors are independent from each other and predictors (x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34de326-75c8-4c51-be8b-323691cbb191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d44e5c0-660d-408f-943a-9750e6601599",
   "metadata": {},
   "source": [
    "The slope is interpreted as the change of y for a one unit increase in x. This is the same idea for the interpretation of the slope of the regression line. Î² ^ 1 represents the estimated increase in Y per unit increase in X. The increase may be negative which is reflected when is negative.\n",
    "\n",
    "e.g. If you know that renting a car costs Rs.25 per day plus Rs.100 deposit, you can write the equation as y = 25x + 100, where y is the cost and x is the number of days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d88554-c8e9-4510-88cf-cbbc32b5c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad698a-2726-4c61-987b-713eb5ff9ef4",
   "metadata": {},
   "source": [
    "Gradient Descent is an iterative optimization algorithm that tries to find the optimum value (Minimum/Maximum) of an objective function. It is one of the most used optimization techniques in machine learning projects for updating the parameters of a model in order to minimize a cost function.  \n",
    "\n",
    "The main aim of gradient descent is to find the best parameters of a model which gives the highest accuracy on training as well as testing datasets. In gradient descent, The gradient is a vector that points in the direction of the steepest increase of the function at a specific point. Moving in the opposite direction of the gradient allows the algorithm to gradually descend towards lower values of the function, and eventually reaching to the minimum of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b32525fa-7fc8-4d9e-b121-c448ab614dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428c520-a7dc-4145-aa1e-7287f564ddfc",
   "metadata": {},
   "source": [
    "Multiple regression works by considering the values of the available multiple independent variables and predicting the value of one dependent variable. Example: A researcher decides to study students' performance from a school over a period of time.\n",
    "\n",
    "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and two or more x variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8b956dc-a2c2-4beb-a31f-0fc1b3dd51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2af5c-c868-4a2c-a6a7-16570a0d032e",
   "metadata": {},
   "source": [
    "In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\n",
    "\n",
    "One method to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable, and a VIF value greater than 1.5 indicates multicollinearity.\n",
    "\n",
    "How to Deal with Multicollinearity\n",
    "1) Remove some of the highly correlated independent variables.\n",
    "2) Linearly combine the independent variables, such as adding them together.\n",
    "3) Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ff0549-5311-4ca0-996b-1d2bbfae0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23b5fa0-07d1-4d1a-a63f-59df0abe1353",
   "metadata": {},
   "source": [
    "A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship.\n",
    "\n",
    "Linear regression is appropriate for modeling linear relationships between variables, polynomial regression is used for modeling non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1135c5-e329-4776-a90a-e8551e54aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b948f18-10ff-4582-b7e7-ff047727d921",
   "metadata": {},
   "source": [
    "Advantages :\n",
    "\n",
    "Polynomial provides the best approximation of the relationship between the dependent and independent variable. A Broad range of function can be fit under it. Polynomial basically fits a wide range of curvature.\n",
    "\n",
    "Disadvantages :\n",
    "\n",
    "One or two outliers in the data might have a significant impact on the nonlinear analysis' outcomes. These are overly reliant on outliers. Furthermore, there are fewer model validation methods for detecting outliers in nonlinear regression than there are for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d29c9c-fcd9-420b-a06d-3f6c93e9421f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
