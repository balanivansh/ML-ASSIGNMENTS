{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb9aa16-63e8-4693-9a89-226537147de2",
   "metadata": {},
   "source": [
    "## 1.\n",
    "Linear regression provides a continuous output but Logistic regression provides discreet output. The purpose of Linear Regression is to find the best-fitted line while Logistic regression is one step ahead and fitting the line values to the sigmoid curve.\n",
    "\n",
    "some examples where logistic regression is more appropriate :\n",
    "1) redict whether or not a customer will default on a loan.\n",
    "2) Predict whether or not a patient will have a heart attack.\n",
    "3) Predict whether or not an email is a spam.\n",
    "4) Predict whether or not a student will pass/fail an exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4bf1da-3d0d-4f6a-9453-6213f68e271d",
   "metadata": {},
   "source": [
    "## 2.\n",
    "The cost function always approaches its global minimum during training. The logistic regression algorithm uses the sigmoid function, which is exponential.\n",
    "\n",
    "This optimization takes place through algorithmic methods for learning. The method most commonly used for logistic regression is gradient descent. Gradient descent requires convex cost functions. Mean Squared Error, commonly used for linear regression models, isn't convex for logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf6f4df-a325-4345-9ffe-e58a0c9c1da1",
   "metadata": {},
   "source": [
    "## 3.\n",
    "Regularization is used to reduce the complexity of the prediction function by imposing a penalty.\n",
    "\n",
    "Regularization helps you avoid overfitting by adding a penalty term to the cost function of your model, which measures how well your model fits the data. The penalty term reduces the complexity of your model by shrinking or eliminating some of the coefficients of your input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe9f22-82d5-4143-a437-f1250e321774",
   "metadata": {},
   "source": [
    "## 4.\n",
    "ROC curves in logistic regression are used for determining the best cutoff value for predicting whether a new observation is a \"failure\" (0) or a \"success\" (1).\n",
    "\n",
    "The ROC curve is produced by calculating and plotting the true positive rate against the false positive rate for a single classifier at a variety of thresholds. For example, in logistic regression, the threshold would be the predicted probability of an observation belonging to the positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd80d07d-c94c-4d22-8408-bee6b309125a",
   "metadata": {},
   "source": [
    "## 5.\n",
    "\n",
    "a) Chi-square Test\n",
    "\n",
    "The Chi-square test is used for categorical features in a dataset. We calculate Chi-square between each feature and the target and select the desired number of features with the best Chi-square scores.\n",
    "\n",
    "b) Fisher’s Score\n",
    "\n",
    "Fisher score is one of the most widely used supervised feature selection methods. The algorithm we will use returns the ranks of the variables based on the fisher’s score in descending order. \n",
    "\n",
    "c) Correlation Coefficient\n",
    "\n",
    "Correlation is a measure of the linear relationship between 2 or more variables. Through correlation, we can predict one variable from the other. The logic behind using correlation for feature selection is that good variables correlate highly with the target. \n",
    "\n",
    "d) Variance Threshold\n",
    "\n",
    "The variance threshold is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e., features with the same value in all samples. \n",
    "\n",
    "e) Mean Absolute Difference (MAD)\n",
    "\n",
    "‘The mean absolute difference (MAD) computes the absolute difference from the mean value. The main difference between the variance and MAD measures is the absence of the square in the latter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c67d9-4e88-4492-883b-f02d27027866",
   "metadata": {},
   "source": [
    "## 6.\n",
    "Logistic regression does not support imbalanced classification directly. Instead, the training algorithm used to fit the logistic regression model must be modified to take the skewed distribution into account.\n",
    "\n",
    "We need to specify class importance using a dictionary with the key as a value for each class weight. To adjust the class weight for an imbalanced dataset using the sklearn LogisticRegression function, we could specify class_weight='balanced'. It will balance the classes automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478317fc-b296-417a-afbf-ebb8b609ff70",
   "metadata": {},
   "source": [
    "## 7.\n",
    "Chalenges faced while implementing logistic regression :\n",
    "\n",
    "1) Logistic regression fails to predict a continuous outcome. ...\n",
    "2) Logistic regression assumes linearity between the predicted (dependent) variable and the predictor (independent) variables. ...\n",
    "3) Logistic regression may not be accurate if the sample size is too small.\n",
    "\n",
    "Factors we look to classify problems :\n",
    "1. It assumes that there is minimal or no multicollinearity among the independent variables i.e, predictors are not correlated.\n",
    "2. There should be a linear relationship between the logit of the outcome and each predictor variable.\n",
    "\n",
    "How to check for multicollinearity in regression model:\n",
    "\n",
    "The general rule of thumb is that if simple correlation coefficient between two regressors is greater than 0.8 or 0.9, the multicollinearity is a serious problem. Multicollinearity does not reduce the predictive power or reliability of the model as a whole; it only affects calculations regarding individual predictors.\n",
    "\n",
    "To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc491e1b-a44f-4968-9c77-060584fa4e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
